import torch
from PIL import Image
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration, set_seed
from peft import PeftModel
import argparse
import random
import numpy as np
import os
import logging
import json
import time

# è¨­ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

def set_random_seed(seed=43):
    """
    è¨­ç½®æ‰€æœ‰éš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿æ¸¬è©¦ç’°å¢ƒèˆ‡è¨“ç·´ç’°å¢ƒç›¸åŒ
    
    Args:
        seed (int): éš¨æ©Ÿç¨®å­å€¼ï¼Œé»˜èªç‚º43ï¼ˆèˆ‡è¨“ç·´è…³æœ¬ä¸€è‡´ï¼‰
    """
    logger.info(f"ğŸ² è¨­ç½®éš¨æ©Ÿç¨®å­: {seed}")
    
    # Pythonå…§ç½®randomæ¨¡å¡Š
    random.seed(seed)
    
    # NumPyéš¨æ©Ÿç¨®å­
    np.random.seed(seed)
    
    # PyTorchéš¨æ©Ÿç¨®å­
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # å¦‚æœä½¿ç”¨å¤šGPU
    
    # Transformersåº«çš„éš¨æ©Ÿç¨®å­
    set_seed(seed)
    
    # ç¢ºä¿CUDAæ“ä½œçš„ç¢ºå®šæ€§ï¼ˆèˆ‡è¨“ç·´æ™‚ä¿æŒä¸€è‡´ï¼‰
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # è¨­ç½®ç’°å¢ƒè®Šé‡
    os.environ['PYTHONHASHSEED'] = str(seed)
    
    logger.info("âœ… éš¨æ©Ÿç¨®å­è¨­ç½®å®Œæˆï¼Œæ¸¬è©¦ç’°å¢ƒå·²èˆ‡è¨“ç·´ç’°å¢ƒåŒæ­¥")

def load_training_seed_if_available(model_dir):
    """
    å˜—è©¦å¾è¨“ç·´å¥½çš„æ¨¡å‹ç›®éŒ„ä¸­åŠ è¼‰è¨“ç·´æ™‚ä½¿ç”¨çš„ç¨®å­
    
    Args:
        model_dir (str): æ¨¡å‹ç›®éŒ„è·¯å¾‘
        
    Returns:
        int: è¨“ç·´æ™‚ä½¿ç”¨çš„ç¨®å­ï¼Œå¦‚æœæ‰¾ä¸åˆ°å‰‡è¿”å›é»˜èªå€¼43
    """
    # å˜—è©¦å¾training_config.jsonåŠ è¼‰ç¨®å­
    config_path = os.path.join(model_dir, "training_config.json")
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                training_seed = config.get('final_seed_used', config.get('seed', 43))
                logger.info(f"ğŸ“ å¾è¨“ç·´é…ç½®({model_dir})ä¸­æ‰¾åˆ°ç¨®å­: {training_seed}")
                return training_seed
        except Exception as e:
            logger.warning(f"è®€å–è¨“ç·´é…ç½®å¤±æ•—({model_dir}): {e}")
    
    # å˜—è©¦å¾random_states.pklåŠ è¼‰ç¨®å­ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    seed_state_path = os.path.join(model_dir, "random_states.pkl")
    if os.path.exists(seed_state_path):
        try:
            import pickle
            with open(seed_state_path, 'rb') as f:
                random_states = pickle.load(f)
                training_seed = random_states.get('seed_used', 43)
                logger.info(f"ğŸ“ å¾éš¨æ©Ÿç‹€æ…‹æ–‡ä»¶({model_dir})ä¸­æ‰¾åˆ°ç¨®å­: {training_seed}")
                return training_seed
        except Exception as e:
            logger.warning(f"è®€å–éš¨æ©Ÿç‹€æ…‹æ–‡ä»¶å¤±æ•—({model_dir}): {e}")
    
    logger.info(f"ğŸ“ æœªæ‰¾åˆ°è¨“ç·´æ™‚çš„ç¨®å­é…ç½®({model_dir})ï¼Œä½¿ç”¨é»˜èªç¨®å­: 43")
    return 43

def load_training_config(model_dir):
    """
    åŠ è¼‰è¨“ç·´é…ç½®ä¿¡æ¯
    
    Args:
        model_dir (str): æ¨¡å‹ç›®éŒ„è·¯å¾‘
        
    Returns:
        dict: è¨“ç·´é…ç½®ä¿¡æ¯
    """
    config_path = os.path.join(model_dir, "training_config.json")
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                return config
        except Exception as e:
            logger.warning(f"è®€å–è¨“ç·´é…ç½®å¤±æ•—: {e}")
    return {}

def parse_args():
    parser = argparse.ArgumentParser(description='æ¯”è¼ƒåŸºç¤æ¨¡å‹å’Œè¨“ç·´å¾Œæ¨¡å‹åœ¨å–®å¼µåœ–ç‰‡ä¸Šçš„è¡¨ç¾ (æ”¯æŒLoRAå’Œå®Œæ•´æ¨¡å‹)')
    parser.add_argument('--base_model_path', type=str, default="/home/itrib30156/llm_vision/qwen3b",
                        help='åŸºç¤æ¨¡å‹è·¯å¾‘')
    parser.add_argument('--lora_model_path', type=str, default="/home/itrib30156/llm_vision/outputs/counting-model_11/checkpoint-370",
                        help='è¨“ç·´å¾Œæ¨¡å‹è·¯å¾‘ï¼ˆæ”¯æŒLoRAé©é…å™¨æˆ–å®Œæ•´å¾®èª¿æ¨¡å‹ï¼‰ã€‚å¦‚æœä¸æŒ‡å®šï¼Œåªæ¸¬è©¦åŸºç¤æ¨¡å‹')
    parser.add_argument('--image_path', type=str, default="/home/itrib30156/llm_vision/self_grpo/images/R.jpg",
                        help='æ¸¬è©¦åœ–ç‰‡è·¯å¾‘')
    parser.add_argument('--question', type=str, default="åœ–ä¸­å…±æœ‰å¤šå°‘ç“¶è£å’–å•¡è±†çš„ç“¶å­?,",
                        help='æ¸¬è©¦å•é¡Œ')
    
    # éš¨æ©Ÿç¨®å­ç›¸é—œåƒæ•¸
    parser.add_argument('--seed', type=int, default=None,
                        help='éš¨æ©Ÿç¨®å­å€¼ã€‚å¦‚æœä¸æŒ‡å®šï¼Œæœƒå˜—è©¦å¾è¨“ç·´æ¨¡å‹ç›®éŒ„è®€å–è¨“ç·´æ™‚çš„ç¨®å­')
    parser.add_argument('--deterministic', action='store_true', default=True,
                        help='å•Ÿç”¨ç¢ºå®šæ€§æ¨¡å¼ï¼Œèˆ‡è¨“ç·´ç’°å¢ƒä¿æŒä¸€è‡´')
    parser.add_argument('--auto_load_seed', action='store_true', default=True,
                        help='è‡ªå‹•å¾è¨“ç·´æ¨¡å‹ç›®éŒ„åŠ è¼‰è¨“ç·´æ™‚ä½¿ç”¨çš„ç¨®å­')
    
    # ç”Ÿæˆåƒæ•¸
    parser.add_argument('--max_new_tokens', type=int, default=128,
                        help='æœ€å¤§ç”Ÿæˆtokenæ•¸')
    parser.add_argument('--temperature', type=float, default=0.6,
                        help='ç”Ÿæˆæº«åº¦')
    parser.add_argument('--do_sample', action='store_true', default=True,
                        help='æ˜¯å¦ä½¿ç”¨é‡‡æ¨£ç”Ÿæˆ')
    parser.add_argument('--top_p', type=float, default=0.9,
                        help='nucleus samplingçš„på€¼')
    parser.add_argument('--top_k', type=int, default=50,
                        help='top-k samplingçš„kå€¼')
    
    # æ¸¬è©¦æ§åˆ¶åƒæ•¸
    parser.add_argument('--compare_mode', action='store_true', default=False,
                        help='æ¯”è¼ƒæ¨¡å¼ï¼šåŒæ™‚æ¸¬è©¦åŸºç¤æ¨¡å‹å’Œè¨“ç·´å¾Œæ¨¡å‹')
    parser.add_argument('--only_lora', action='store_true', default=False,
                        help='åªæ¸¬è©¦è¨“ç·´å¾Œæ¨¡å‹ï¼ˆLoRAæˆ–å®Œæ•´æ¨¡å‹ï¼‰')
    parser.add_argument('--only_base', action='store_true', default=False,
                        help='åªæ¸¬è©¦åŸºç¤æ¨¡å‹')
    
    # å…¶ä»–åƒæ•¸
    parser.add_argument('--verbose', action='store_true', default=False,
                        help='é¡¯ç¤ºè©³ç´°ä¿¡æ¯')
    parser.add_argument('--save_result', type=str, default=None,
                        help='ä¿å­˜æ¸¬è©¦çµæœåˆ°æŒ‡å®šæ–‡ä»¶')
    parser.add_argument('--use_answer_guided_prompt', action='store_true', default=False,
                        help='ä½¿ç”¨ç­”æ¡ˆå¼•å°çš„ç³»çµ±æç¤ºè©ï¼ˆèˆ‡è¨“ç·´æ™‚ä¸€è‡´ï¼‰')
    
    return parser.parse_args()

def get_system_prompt(use_answer_guided=False):
    """ç²å–ç³»çµ±æç¤ºè©"""
    if use_answer_guided:
        # ç­”æ¡ˆå¼•å°æ¨¡å¼çš„ç³»çµ±æç¤ºè©ï¼ˆèˆ‡è¨“ç·´æ™‚ä¸€è‡´ï¼‰
        return (
            "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„è¦–è¦ºåˆ†æåŠ©æ‰‹ï¼Œç‰¹åˆ¥æ“…é•·ç²¾ç¢ºçµ±è¨ˆåœ–ç‰‡ä¸­çš„ç‰©é«”æ•¸é‡ã€‚"
            "è«‹éµå¾ªä»¥ä¸‹æ­¥é©Ÿå›ç­”ï¼š\n"
            "1. åœ¨ <think> </think> æ¨™ç±¤ä¸­é€²è¡Œè©³ç´°çš„è§€å¯Ÿå’Œæ¨ç†ï¼š\n"
            "   - ä»”ç´°æè¿°ä½ åœ¨åœ–ç‰‡ä¸­çœ‹åˆ°çš„å…§å®¹\n"
            "   - èªªæ˜æ¯å€‹ç‰©é«”çš„ä½ç½®å’Œç‰¹å¾µ\n"
            "   - é¢å°è¨ˆæ•¸å•é¡Œå¿…é ˆé€æ­¥é€²è¡Œè¨ˆæ•¸ï¼Œå¯ä»¥ç”¨ç·¨è™Ÿæ–¹å¼ï¼ˆå¦‚ï¼šç¬¬1å€‹...ç¬¬2å€‹...ï¼‰\n"
            "   - è§£é‡‹ä½ çš„è¨ˆæ•¸é‚è¼¯æˆ–åˆ¤æ–·æ–¹æ³•\n"
            "2. åœ¨ <answer> </answer> æ¨™ç±¤ä¸­çµ¦å‡ºæœ€çµ‚çš„æ•¸é‡\n\n"
            "è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œæ•¸å­—è«‹ä½¿ç”¨é˜¿æ‹‰ä¼¯æ•¸å­—ã€‚"
        )
    else:
        # åŸºç¤ç³»çµ±æç¤ºè©
        return "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„è¦–è¦ºåˆ†æåŠ©æ‰‹ï¼Œç‰¹åˆ¥æ“…é•·ç²¾ç¢ºçµ±è¨ˆåœ–ç‰‡ä¸­çš„ç‰©é«”æ•¸é‡ã€‚è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œæ•¸å­—è«‹ä½¿ç”¨é˜¿æ‹‰ä¼¯æ•¸å­—ã€‚"

def load_models(args):
    """
    åŠ è¼‰åŸºç¤æ¨¡å‹å’Œè¨“ç·´å¾Œçš„æ¨¡å‹ï¼ˆæ”¯æŒLoRAå’Œå®Œæ•´æ¨¡å‹ï¼‰
    
    Returns:
        tuple: (processor, base_model, trained_model, training_config, model_type)
    """
    print("ğŸ”§ æ­£åœ¨åŠ è¼‰è™•ç†å™¨å’Œæ¨¡å‹...")
    
    # åŠ è¼‰è™•ç†å™¨
    processor = AutoProcessor.from_pretrained(args.base_model_path, trust_remote_code=True)
    
    # åŠ è¼‰åŸºç¤æ¨¡å‹
    print(f"ğŸ“¦ åŠ è¼‰åŸºç¤æ¨¡å‹: {args.base_model_path}")
    base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        args.base_model_path,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    ).eval()
    
    trained_model = None
    training_config = {}
    model_type = "none"
    
    # åŠ è¼‰è¨“ç·´å¾Œçš„æ¨¡å‹ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.lora_model_path and not args.only_base:
        print(f"ğŸ¯ æª¢æŸ¥è¨“ç·´æ¨¡å‹é¡å‹: {args.lora_model_path}")
        
        # é¦–å…ˆæª¢æŸ¥æ˜¯å¦å­˜åœ¨å¿…è¦æ–‡ä»¶
        if not os.path.exists(args.lora_model_path):
            logger.error(f"è¨“ç·´æ¨¡å‹è·¯å¾‘ä¸å­˜åœ¨: {args.lora_model_path}")
            return processor, base_model, None, {}, "none"
        
        # æª¢æŸ¥æ¨¡å‹é¡å‹
        adapter_config_path = os.path.join(args.lora_model_path, "adapter_config.json")
        full_model_config_path = os.path.join(args.lora_model_path, "config.json")
        
        # åŠ è¼‰è¨“ç·´é…ç½®ä¿¡æ¯
        training_config = load_training_config(args.lora_model_path)
        if training_config:
            print(f"ğŸ“‹ ç™¼ç¾è¨“ç·´é…ç½®:")
            print(f"   - è¨“ç·´æ¨¡å¼: {training_config.get('training_mode', 'æœªçŸ¥')}")
            print(f"   - LoRAç­–ç•¥: {training_config.get('lora_strategy', 'æœªçŸ¥')}")
            print(f"   - å­¸ç¿’ç‡: {training_config.get('learning_rate', 'æœªçŸ¥')}")
            print(f"   - è¨“ç·´è¼ªæ•¸: {training_config.get('num_epochs', 'æœªçŸ¥')}")
            if 'guidance_enabled' in training_config:
                print(f"   - ç­”æ¡ˆå¼•å°: {'æ˜¯' if training_config['guidance_enabled'] else 'å¦'}")
        
        # æ–¹æ³•1: å˜—è©¦ä½œç‚ºGRPOæ¨¡å‹åŠ è¼‰
        if os.path.exists(adapter_config_path):
            print("ğŸ¯ æª¢æ¸¬åˆ°GRPOæ¨¡å‹æ–‡ä»¶ï¼Œå˜—è©¦ä½œç‚ºLoRAé©é…å™¨åŠ è¼‰...")
            try:
                from peft import PeftModel
                trained_model = PeftModel.from_pretrained(
                    base_model,
                    args.lora_model_path,
                    torch_dtype=torch.bfloat16,
                ).eval()
                
                model_type = "lora"
                print(f"âœ… GRPOæ¨¡å‹åŠ è¼‰æˆåŠŸ")
                
                # é¡¯ç¤ºGRPOæ¨¡å‹ä¿¡æ¯
                if hasattr(trained_model, 'peft_config') and trained_model.peft_config:
                    peft_config = trained_model.peft_config[list(trained_model.peft_config.keys())[0]]
                    print(f"ğŸ¯ LoRAé…ç½®ä¿¡æ¯:")
                    print(f"   - LoRAç§© (r): {peft_config.r}")
                    print(f"   - LoRA alpha: {peft_config.lora_alpha}")
                    print(f"   - LoRA dropout: {peft_config.lora_dropout}")
                    print(f"   - ç›®æ¨™æ¨¡çµ„: {len(peft_config.target_modules)} å€‹")
                    if args.verbose:
                        print(f"   - ç›®æ¨™æ¨¡çµ„åˆ—è¡¨: {peft_config.target_modules}")
                
            except Exception as e:
                print(f"âŒ ä½œç‚ºGRPOæ¨¡å‹åŠ è¼‰å¤±æ•—: {e}")
                print("ğŸ”„ å˜—è©¦ä½œç‚ºå®Œæ•´æ¨¡å‹åŠ è¼‰...")
                trained_model = None
        
        # æ–¹æ³•2: ä½œç‚ºå®Œæ•´æ¨¡å‹åŠ è¼‰
        if trained_model is None and os.path.exists(full_model_config_path):
            print("ğŸ“¦ æª¢æ¸¬åˆ°å®Œæ•´æ¨¡å‹æ–‡ä»¶ï¼Œä½œç‚ºå®Œæ•´æ¨¡å‹åŠ è¼‰...")
            try:
                trained_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                    args.lora_model_path,
                    device_map="auto",
                    torch_dtype=torch.bfloat16,
                    trust_remote_code=True
                ).eval()
                
                model_type = "full_model"
                print(f"âœ… å®Œæ•´è¨“ç·´æ¨¡å‹åŠ è¼‰æˆåŠŸ")
                
                # æ¯”è¼ƒåƒæ•¸é‡
                base_params = sum(p.numel() for p in base_model.parameters())
                trained_params = sum(p.numel() for p in trained_model.parameters())
                print(f"ğŸ“Š æ¨¡å‹åƒæ•¸å°æ¯”:")
                print(f"   - åŸºç¤æ¨¡å‹åƒæ•¸: {base_params:,}")
                print(f"   - è¨“ç·´æ¨¡å‹åƒæ•¸: {trained_params:,}")
                
                if abs(base_params - trained_params) < 1000:  # åƒæ•¸é‡åŸºæœ¬ç›¸åŒ
                    print(f"   - âœ… åƒæ•¸é‡ä¸€è‡´ï¼Œç¢ºèªç‚ºå¾®èª¿å¾Œçš„å®Œæ•´æ¨¡å‹")
                else:
                    print(f"   - âš ï¸ åƒæ•¸é‡å·®ç•°: {abs(base_params - trained_params):,}")
                
            except Exception as e:
                print(f"âŒ ä½œç‚ºå®Œæ•´æ¨¡å‹åŠ è¼‰å¤±æ•—: {e}")
                trained_model = None
        
        # å¦‚æœå…©ç¨®æ–¹æ³•éƒ½å¤±æ•—
        if trained_model is None:
            print("âŒ ç„¡æ³•è­˜åˆ¥è¨“ç·´æ¨¡å‹æ ¼å¼ï¼Œå°‡åªæ¸¬è©¦åŸºç¤æ¨¡å‹")
            model_type = "failed"
        
    return processor, base_model, trained_model, training_config, model_type

def generate_response(model, processor, prompt, image, args, model_name="Model"):
    """
    ç”Ÿæˆæ¨¡å‹å›ç­”
    
    Args:
        model: è¦æ¸¬è©¦çš„æ¨¡å‹
        processor: è™•ç†å™¨
        prompt: æç¤ºè©
        image: åœ–ç‰‡
        args: å‘½ä»¤è¡Œåƒæ•¸
        model_name: æ¨¡å‹åç¨±ï¼ˆç”¨æ–¼æ—¥å¿—é¡¯ç¤ºï¼‰
    
    Returns:
        tuple: (response_text, generation_time)
    """
    print(f"ğŸ§  {model_name} æ­£åœ¨ç”Ÿæˆå›ç­”...")
    
    # è™•ç†æç¤º
    text = processor.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)
    inputs = processor(
        text=[text],
        images=[image],
        return_tensors="pt",
    )
    
    # å°‡è¼¸å…¥ç§»è‡³é©ç•¶çš„è¨­å‚™
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    # ç”Ÿæˆåƒæ•¸è¨­ç½®
    generation_kwargs = {
        'max_new_tokens': args.max_new_tokens,
        'do_sample': args.do_sample,
    }
    
    # åªæœ‰åœ¨æ¡æ¨£æ¨¡å¼ä¸‹æ‰æ·»åŠ ç›¸é—œåƒæ•¸
    if args.do_sample:
        generation_kwargs.update({
            'temperature': args.temperature,
            'top_p': args.top_p,
            'top_k': args.top_k,
        })
    else:
        # éæ¡æ¨£æ¨¡å¼ä¸‹å¯ä»¥ä½¿ç”¨çš„åƒæ•¸
        generation_kwargs.update({
            'num_beams': 1,  # ç¢ºä¿æ˜¯è²ªå©ªè§£ç¢¼
        })
    
    # è¨˜éŒ„ç”Ÿæˆæ™‚é–“
    start_time = time.time()
    
    # ç”Ÿæˆå›ç­”
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            **generation_kwargs
        )
    
    generation_time = time.time() - start_time
    
    # è§£ç¢¼å›ç­”
    response = processor.batch_decode(outputs, skip_special_tokens=True)[0]
    # åªç²å–æ¨¡å‹ç”Ÿæˆçš„éƒ¨åˆ†
    response = response.split("assistant:")[-1].strip()
    
    return response, generation_time

def analyze_response_structure(response):
    """
    åˆ†æå›ç­”çµæ§‹
    
    Args:
        response (str): æ¨¡å‹å›ç­”
        
    Returns:
        dict: åˆ†æçµæœ
    """
    import re
    
    analysis = {
        'has_think_tag': False,
        'has_answer_tag': False,
        'think_content': '',
        'answer_content': '',
        'think_length': 0,
        'total_length': len(response),
        'has_enumeration': False,
        'number_count': 0
    }
    
    # æª¢æŸ¥thinkæ¨™ç±¤
    think_match = re.search(r'<think>(.*?)</think>', response, re.DOTALL)
    if think_match:
        analysis['has_think_tag'] = True
        analysis['think_content'] = think_match.group(1).strip()
        analysis['think_length'] = len(analysis['think_content'])
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æšèˆ‰
        analysis['has_enumeration'] = bool(re.search(r'[1-9]\s*[.ã€)ï¼‰]', analysis['think_content']))
    
    # æª¢æŸ¥answeræ¨™ç±¤
    answer_match = re.search(r'<answer>(.*?)</answer>', response, re.DOTALL)
    if answer_match:
        analysis['has_answer_tag'] = True
        analysis['answer_content'] = answer_match.group(1).strip()
    
    # çµ±è¨ˆæ•¸å­—å‡ºç¾æ¬¡æ•¸
    numbers = re.findall(r'\d+', response)
    analysis['number_count'] = len(numbers)
    
    return analysis

def print_comparison_results(base_response, trained_response, base_analysis, trained_analysis, 
                           base_time, trained_time, model_type):
    """æ‰“å°æ¯”è¼ƒçµæœï¼ˆæ”¯æŒä¸åŒæ¨¡å‹é¡å‹ï¼‰"""
    print("\n" + "="*80)
    print("ğŸ“Š æ¨¡å‹æ¯”è¼ƒçµæœ")
    print("="*80)
    
    # åŸºç¤æ¨¡å‹çµæœ
    print("ğŸ”µ åŸºç¤æ¨¡å‹å›ç­”:")
    print("-"*60)
    print(base_response)
    print(f"\nâ±ï¸  ç”Ÿæˆæ™‚é–“: {base_time:.2f}ç§’")
    
    # è¨“ç·´å¾Œæ¨¡å‹çµæœ
    model_name = "GRPOæ¨¡å‹" if model_type == "lora" else "GRPOæ¨¡å‹"
    model_emoji = "ğŸ¯" if model_type == "lora" else "ğŸš€"
    
    print(f"\n{model_emoji} {model_name}å›ç­”:")
    print("-"*60)
    print(trained_response)
    print(f"\nâ±ï¸  ç”Ÿæˆæ™‚é–“: {trained_time:.2f}ç§’")
    
    # çµæ§‹åŒ–åˆ†ææ¯”è¼ƒ
    print(f"\nğŸ“‹ çµæ§‹åŒ–åˆ†ææ¯”è¼ƒ:")
    print("-"*60)
    print(f"{'é …ç›®':<20} {'åŸºç¤æ¨¡å‹':<15} {f'{model_name}':<15} {'æ”¹é€²':<10}")
    print("-"*60)
    
    base_think = 'æ˜¯' if base_analysis['has_think_tag'] else 'å¦'
    trained_think = 'æ˜¯' if trained_analysis['has_think_tag'] else 'å¦'
    think_improve = 'âœ…' if trained_analysis['has_think_tag'] and not base_analysis['has_think_tag'] else 'â–'
    print(f"{'åŒ…å«æ€è€ƒéç¨‹':<20} {base_think:<15} {trained_think:<15} {think_improve:<10}")
    
    base_answer = 'æ˜¯' if base_analysis['has_answer_tag'] else 'å¦'
    trained_answer = 'æ˜¯' if trained_analysis['has_answer_tag'] else 'å¦'
    answer_improve = 'âœ…' if trained_analysis['has_answer_tag'] and not base_analysis['has_answer_tag'] else 'â–'
    print(f"{'åŒ…å«ç­”æ¡ˆæ¨™ç°½':<20} {base_answer:<15} {trained_answer:<15} {answer_improve:<10}")
    
    length_improve = 'âœ…' if trained_analysis['think_length'] > base_analysis['think_length'] else 'â–'
    print(f"{'æ€è€ƒå…§å®¹é•·åº¦':<20} {base_analysis['think_length']:<15} {trained_analysis['think_length']:<15} {length_improve:<10}")
    
    base_enum = 'æ˜¯' if base_analysis['has_enumeration'] else 'å¦'
    trained_enum = 'æ˜¯' if trained_analysis['has_enumeration'] else 'å¦'
    enum_improve = 'âœ…' if trained_analysis['has_enumeration'] and not base_analysis['has_enumeration'] else 'â–'
    print(f"{'åŒ…å«æšèˆ‰è¨ˆæ•¸':<20} {base_enum:<15} {trained_enum:<15} {enum_improve:<10}")
    
    total_improve = 'âœ…' if trained_analysis['total_length'] > base_analysis['total_length'] else 'â–'
    print(f"{'ç¸½å›ç­”é•·åº¦':<20} {base_analysis['total_length']:<15} {trained_analysis['total_length']:<15} {total_improve:<10}")
    
    time_improve = 'âœ…' if trained_time < base_time else 'â–'
    print(f"{'ç”Ÿæˆæ™‚é–“(ç§’)':<20} {base_time:.2f}<{'':<14} {trained_time:.2f}<{'':<14} {time_improve:<10}")
    
    # æœ€çµ‚ç­”æ¡ˆæå–
    if base_analysis['has_answer_tag'] and trained_analysis['has_answer_tag']:
        print(f"\nğŸ¯ æœ€çµ‚ç­”æ¡ˆæ¯”è¼ƒ:")
        print(f"  åŸºç¤æ¨¡å‹: {base_analysis['answer_content']}")
        print(f"  {model_name}: {trained_analysis['answer_content']}")
        if base_analysis['answer_content'] != trained_analysis['answer_content']:
            print("  âš ï¸  ç­”æ¡ˆä¸åŒï¼")
        else:
            print("  âœ… ç­”æ¡ˆä¸€è‡´")
    
    # æ¨¡å‹é¡å‹èªªæ˜
    print(f"\nğŸ“ æ¨¡å‹é¡å‹èªªæ˜:")
    if model_type == "lora":
        print("  ğŸ¯ LoRAé©é…å™¨: åœ¨åŸºç¤æ¨¡å‹ä¸Šé™„åŠ çš„è¼•é‡ç´šé©é…å™¨")
        print("  ğŸ“Š å„ªå‹¢: åƒæ•¸æ•ˆç‡é«˜ï¼Œè¨“ç·´å¿«é€Ÿï¼Œå­˜å„²å ç”¨å°")
    elif model_type == "full_model":
        print("  ğŸš€ å®Œæ•´å¾®èª¿æ¨¡å‹: åŸºç¤æ¨¡å‹çš„æ‰€æœ‰åƒæ•¸éƒ½ç¶“éè¨“ç·´æ›´æ–°")
        print("  ğŸ“Š å„ªå‹¢: å¯èƒ½æœ‰æ›´å¼·çš„è¡¨ç¾ï¼Œä½†éœ€è¦æ›´å¤šå­˜å„²ç©ºé–“")
        print("  âš ï¸ æ³¨æ„: å®Œæ•´æ¨¡å‹å¯èƒ½éæ“¬åˆï¼Œéœ€è¦ä»”ç´°è©•ä¼°æ³›åŒ–èƒ½åŠ›")

def main():
    args = parse_args()
    
    print("ğŸš€ å•Ÿå‹• Qwen VL æ¨¡å‹æ¯”è¼ƒæ¸¬è©¦ï¼ˆæ”¯æŒå®Œæ•´æ¨¡å‹ï¼‰")
    print("=" * 80)
    
    # [ç¨®å­è¨­ç½®éƒ¨åˆ†ä¿æŒä¸è®Š...]
    if args.seed is not None:
        final_seed = args.seed
        logger.info(f"ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„ç¨®å­: {final_seed}")
    elif args.lora_model_path and args.auto_load_seed:
        final_seed = load_training_seed_if_available(args.lora_model_path)
        logger.info(f"æª¢æ¸¬åˆ°è¨“ç·´æ¨¡å‹ï¼Œä½¿ç”¨è¨“ç·´æ™‚çš„ç¨®å­ä»¥ç¢ºä¿ä¸€è‡´æ€§: {final_seed}")
    elif args.auto_load_seed:
        final_seed = load_training_seed_if_available(args.base_model_path)
    else:
        final_seed = 43
        logger.info(f"ä½¿ç”¨é»˜èªç¨®å­: {final_seed}")
    
    set_random_seed(final_seed)
    
    print(f"ğŸ² æ¸¬è©¦ç’°å¢ƒç¨®å­: {final_seed}")
    print(f"ğŸ”§ ç¢ºå®šæ€§æ¨¡å¼: {'å•Ÿç”¨' if args.deterministic else 'ç¦ç”¨'}")
    
    # é¡¯ç¤ºæ¸¬è©¦æ¨¡å¼
    if args.only_base:
        print("ğŸ”µ æ¸¬è©¦æ¨¡å¼: åƒ…åŸºç¤æ¨¡å‹")
    elif args.only_lora:
        print("ğŸ¯ æ¸¬è©¦æ¨¡å¼: åƒ…è¨“ç·´å¾Œæ¨¡å‹")
    else:
        print("âš–ï¸  æ¸¬è©¦æ¨¡å¼: æ¯”è¼ƒåŸºç¤æ¨¡å‹èˆ‡è¨“ç·´å¾Œæ¨¡å‹")
    
    print("-" * 80)
    
    # åŠ è¼‰æ¨¡å‹ï¼ˆæ–°çš„æ–¹æ³•ï¼‰
    processor, base_model, trained_model, training_config, model_type = load_models(args)
    
    # æ ¹æ“šåŠ è¼‰çµæœé¡¯ç¤ºä¿¡æ¯
    if model_type == "lora":
        print("âœ… æˆåŠŸåŠ è¼‰LoRAé©é…å™¨æ¨¡å‹")
    elif model_type == "full_model":
        print("âœ… æˆåŠŸåŠ è¼‰å®Œæ•´è¨“ç·´æ¨¡å‹")
        print("â„¹ï¸ æ³¨æ„: é€™æ˜¯ä¸€å€‹å®Œæ•´çš„å¾®èª¿æ¨¡å‹ï¼Œè€Œä¸æ˜¯LoRAé©é…å™¨")
    elif model_type == "failed":
        print("âŒ è¨“ç·´æ¨¡å‹åŠ è¼‰å¤±æ•—ï¼Œå°‡åªæ¸¬è©¦åŸºç¤æ¨¡å‹")
    
    # [åœ–ç‰‡åŠ è¼‰å’Œæ¸¬è©¦éƒ¨åˆ†ä¿æŒä¸è®Š...]
    print(f"ğŸ–¼ï¸  åŠ è¼‰åœ–ç‰‡: {args.image_path}")
    try:
        image = Image.open(args.image_path).convert("RGB")
        print(f"ğŸ“ åœ–ç‰‡å°ºå¯¸: {image.size}")
    except Exception as e:
        print(f"âŒ åŠ è¼‰åœ–ç‰‡å¤±æ•—: {e}")
        return
    
    question = args.question
    print(f"â“ æ¸¬è©¦å•é¡Œ: {question}")
    
    # æ ¹æ“šè¨“ç·´é…ç½®æˆ–å‘½ä»¤è¡Œåƒæ•¸æ±ºå®šæ˜¯å¦ä½¿ç”¨ç­”æ¡ˆå¼•å°æç¤º
    use_guided_prompt = args.use_answer_guided_prompt
    if not use_guided_prompt and training_config.get('guidance_enabled', False):
        use_guided_prompt = True
        print(f"ğŸ¯ æª¢æ¸¬åˆ°ç­”æ¡ˆå¼•å°è¨“ç·´ï¼Œè‡ªå‹•å•Ÿç”¨ç›¸æ‡‰çš„ç³»çµ±æç¤º")
    
    system_prompt = get_system_prompt(use_guided_prompt)
    
    if args.verbose:
        print(f"\nğŸ“ ç³»çµ±æç¤ºè©:")
        print("-" * 40)
        print(system_prompt)
        print("-" * 40)
    
    # æ§‹å»ºæç¤º
    prompt = [
        {'role': 'system', 'content': [{"type": "text", "text": system_prompt}]},
        {'role': 'user', 'content': [
            {"type": "image"},
            {"type": "text", "text": question}
        ]}
    ]
    
    # é¡¯ç¤ºç”Ÿæˆåƒæ•¸
    print(f"\nâš™ï¸  ç”Ÿæˆåƒæ•¸:")
    print(f"  - max_new_tokens: {args.max_new_tokens}")
    print(f"  - do_sample: {args.do_sample}")
    if args.do_sample:
        print(f"  - temperature: {args.temperature}")
        print(f"  - top_p: {args.top_p}")
        print(f"  - top_k: {args.top_k}")
    else:
        print(f"  - æ¨¡å¼: è²ªå©ªè§£ç¢¼ (greedy decoding)")
    
    # æ¸¬è©¦çµæœå­˜å„²
    results = {
        'seed_used': final_seed,
        'base_model_path': args.base_model_path,
        'trained_model_path': args.lora_model_path,
        'model_type': model_type,
        'image_path': args.image_path,
        'question': question,
        'use_guided_prompt': use_guided_prompt,
        'training_config': training_config,
        'generation_params': {
            'max_new_tokens': args.max_new_tokens,
            'do_sample': args.do_sample,
            'temperature': args.temperature if args.do_sample else None,
            'top_p': args.top_p if args.do_sample else None,
            'top_k': args.top_k if args.do_sample else None,
        }
    }
    
    # æ¸¬è©¦åŸºç¤æ¨¡å‹
    base_response = None
    base_analysis = None
    base_time = None
    
    if not args.only_lora:
        base_response, base_time = generate_response(
            base_model, processor, prompt, image, args, "åŸºç¤æ¨¡å‹"
        )
        base_analysis = analyze_response_structure(base_response)
        results['base_model'] = {
            'response': base_response,
            'generation_time': base_time,
            'analysis': base_analysis
        }
    
    # æ¸¬è©¦è¨“ç·´å¾Œæ¨¡å‹
    trained_response = None
    trained_analysis = None
    trained_time = None
    
    if trained_model is not None and not args.only_base:
        model_name = "GRPOæ¨¡å‹" if model_type == "lora" else "è¨“ç·´å¾Œæ¨¡å‹"
        trained_response, trained_time = generate_response(
            trained_model, processor, prompt, image, args, model_name
        )
        trained_analysis = analyze_response_structure(trained_response)
        results['trained_model'] = {
            'response': trained_response,
            'generation_time': trained_time,
            'analysis': trained_analysis,
            'model_type': model_type
        }
    
    # é¡¯ç¤ºçµæœ
    if args.compare_mode and base_response and trained_response:
        # æ¯”è¼ƒæ¨¡å¼
        print_comparison_results(base_response, trained_response, base_analysis, 
                               trained_analysis, base_time, trained_time, model_type)
    else:
        # å–®æ¨¡å‹æ¨¡å¼
        if base_response and not args.only_lora:
            print("\n" + "="*60)
            print("ğŸ”µ åŸºç¤æ¨¡å‹å›ç­”:")
            print("-"*60)
            print(base_response)
            print(f"\nâ±ï¸  ç”Ÿæˆæ™‚é–“: {base_time:.2f}ç§’")
            
            if base_analysis['has_think_tag'] or base_analysis['has_answer_tag']:
                print(f"\nğŸ“Š çµæ§‹åˆ†æ:")
                print(f"  âœ… æ€è€ƒéç¨‹: {'æ˜¯' if base_analysis['has_think_tag'] else 'å¦'}")
                print(f"  âœ… çµæ§‹åŒ–å›ç­”: {'æ˜¯' if base_analysis['has_answer_tag'] else 'å¦'}")
                if base_analysis['has_answer_tag']:
                    print(f"  ğŸ¯ æœ€çµ‚ç­”æ¡ˆ: {base_analysis['answer_content']}")
        
        if trained_response and not args.only_base:
            model_name = "GRPOæ¨¡å‹" if model_type == "lora" else "GRPOæ¨¡å‹"
            model_emoji = "ğŸ¯" if model_type == "lora" else "ğŸš€"
            
            print(f"\n" + "="*60)
            print(f"{model_emoji} {model_name}å›ç­”:")
            print("-"*60)
            print(trained_response)
            print(f"\nâ±ï¸  ç”Ÿæˆæ™‚é–“: {trained_time:.2f}ç§’")
            
            if trained_analysis['has_think_tag'] or trained_analysis['has_answer_tag']:
                print(f"\nğŸ“Š çµæ§‹åˆ†æ:")
                print(f"  âœ… æ€è€ƒéç¨‹: {'æ˜¯' if trained_analysis['has_think_tag'] else 'å¦'}")
                print(f"  âœ… çµæ§‹åŒ–å›ç­”: {'æ˜¯' if trained_analysis['has_answer_tag'] else 'å¦'}")
                if trained_analysis['has_answer_tag']:
                    print(f"  ğŸ¯ æœ€çµ‚ç­”æ¡ˆ: {trained_analysis['answer_content']}")
    
    # ä¿å­˜çµæœï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.save_result:
        with open(args.save_result, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ æ¸¬è©¦çµæœå·²ä¿å­˜åˆ°: {args.save_result}")
    
    print(f"\nğŸ¯ æ¸¬è©¦å®Œæˆï¼")
    
    # æ ¹æ“šæ¨¡å‹é¡å‹çµ¦å‡ºä¸åŒçš„ç¸½çµ
    if trained_model and base_response and trained_response:
        print(f"\nğŸ“ˆ è¨“ç·´æ•ˆæœè©•ä¼°:")
        print(f"   ğŸ² æ¸¬è©¦ä¸€è‡´æ€§: åŸºç¤æ¨¡å‹å’Œè¨“ç·´æ¨¡å‹ä½¿ç”¨ç›¸åŒç¨®å­({final_seed})ç¢ºä¿å…¬å¹³æ¯”è¼ƒ")
        
        if model_type == "lora":
            print(f"   ğŸ“Š LoRAé©é…å™¨æ•ˆæœ:")
        else:
            print(f"   ğŸ“Š å®Œæ•´å¾®èª¿æ•ˆæœ:")
        
        if trained_analysis['has_think_tag'] and not base_analysis['has_think_tag']:
            print("   âœ… æˆåŠŸå­¸æœƒçµæ§‹åŒ–æ€è€ƒ")
        if trained_analysis['think_length'] > base_analysis['think_length']:
            print("   âœ… æ¨ç†éç¨‹æ›´åŠ è©³ç´°")
        if trained_analysis['has_enumeration'] and not base_analysis['has_enumeration']:
            print("   âœ… å­¸æœƒäº†é€æ­¥è¨ˆæ•¸")
        if trained_time < base_time:
            print("   âœ… ç”Ÿæˆé€Ÿåº¦æœ‰æ‰€æå‡")
        
        if model_type == "full_model":
            print("   âš ï¸ æ³¨æ„: å®Œæ•´æ¨¡å‹å ç”¨æ›´å¤šå­˜å„²ç©ºé–“ï¼Œè«‹è©•ä¼°æ˜¯å¦å€¼å¾—")

if __name__ == "__main__":
    main()


# ============================================================================
# GRPOæ¨¡å‹æ¯”è¼ƒæ¸¬è©¦ä½¿ç”¨ç¯„ä¾‹ï¼ˆå¸¶éš¨æ©Ÿç¨®å­å›ºå®šï¼‰
# ============================================================================

"""
ğŸ¯ GRPOæ¨¡å‹æ¯”è¼ƒæ¸¬è©¦ç¯„ä¾‹ï¼ˆå„ªåŒ–ç¨®å­ä¸€è‡´æ€§ï¼‰ï¼š

1. å®Œæ•´æ¯”è¼ƒæ¸¬è©¦ï¼ˆè‡ªå‹•ä½¿ç”¨LoRAè¨“ç·´ç¨®å­ç¢ºä¿ä¸€è‡´æ€§ï¼‰:
python test_with_lora_comparison.py \
    --base_model_path "/home/itrib30156/llm_vision/new_qwen3b" \
    --lora_model_path "/home/itrib30156/llm_vision/outputs/Qwen25VL-Counting-GRPO-Guided_3" \
    --image_path "/home/itrib30156/llm_vision/self_grpo/images/R.jpg" \
    --question "åœ–ä¸­æœ‰å¤šå°‘æ«ƒå­æ”¾äº†å’–å•¡è±†?" \
    --verbose
    # æ³¨æ„ï¼šåŸºç¤æ¨¡å‹å’ŒGRPOæ¨¡å‹å°‡è‡ªå‹•ä½¿ç”¨ç›¸åŒçš„è¨“ç·´ç¨®å­é€²è¡Œå…¬å¹³æ¯”è¼ƒ

2. æŒ‡å®šç¨®å­çš„æ¯”è¼ƒæ¸¬è©¦ï¼ˆæ‰‹å‹•ç¢ºä¿ä¸€è‡´æ€§ï¼‰:
python test_with_lora_comparison.py \
    --seed 43 \
    --base_model_path "/path/to/base/model" \
    --lora_model_path "/path/to/lora/model" \
    --image_path "/path/to/test/image.jpg" \
    --use_answer_guided_prompt \
    --save_result "comparison_result.json"
    # æ³¨æ„ï¼šæ‰‹å‹•æŒ‡å®šç¨®å­æœƒè¦†è“‹è‡ªå‹•è®€å–çš„LoRAè¨“ç·´ç¨®å­

3. åƒ…æ¸¬è©¦GRPOæ¨¡å‹ï¼ˆä½¿ç”¨LoRAè¨“ç·´ç¨®å­ï¼‰:
python test_with_lora_comparison.py \
    --only_lora \
    --lora_model_path "/path/to/lora/model" \
    --image_path "/path/to/image.jpg" \
    --max_new_tokens 512
    # æ³¨æ„ï¼šå³ä½¿åªæ¸¬è©¦GRPOæ¨¡å‹ï¼Œä¹Ÿæœƒä½¿ç”¨LoRAè¨“ç·´æ™‚çš„ç¨®å­

4. åƒ…æ¸¬è©¦åŸºç¤æ¨¡å‹ï¼ˆå¦‚æœæœ‰LoRAè·¯å¾‘ï¼Œä»ä½¿ç”¨LoRAç¨®å­ç¢ºä¿å¯æ¯”è¼ƒæ€§ï¼‰:
python test_with_lora_comparison.py \
    --only_base \
    --lora_model_path "/path/to/lora/model" \
    --image_path "/path/to/image.jpg"
    # æ³¨æ„ï¼šå³ä½¿åªæ¸¬è©¦åŸºç¤æ¨¡å‹ï¼Œå¦‚æœæŒ‡å®šäº†lora_model_pathï¼Œ
    # åŸºç¤æ¨¡å‹ä»æœƒä½¿ç”¨LoRAè¨“ç·´æ™‚çš„ç¨®å­ï¼Œä»¥ä¾¿å¾ŒçºŒæ¯”è¼ƒ

5. æ‰¹é‡æ¸¬è©¦ï¼ˆç¢ºä¿ç¨®å­ä¸€è‡´æ€§ï¼‰:
# ç¬¬ä¸€æ¬¡æ¸¬è©¦
python test_with_lora_comparison.py \
    --base_model_path "/path/to/base/model" \
    --lora_model_path "/path/to/lora/model" \
    --image_path "/path/to/image1.jpg" \
    --question "åœ–ä¸­æœ‰å¤šå°‘å€‹è˜‹æœï¼Ÿ" \
    --save_result "test1.json"

# ç¬¬äºŒæ¬¡æ¸¬è©¦ï¼ˆè‡ªå‹•ä½¿ç”¨ç›¸åŒç¨®å­ï¼‰
python test_with_lora_comparison.py \
    --base_model_path "/path/to/base/model" \
    --lora_model_path "/path/to/lora/model" \
    --image_path "/path/to/image2.jpg" \
    --question "åœ–ä¸­æœ‰å¤šå°‘å€‹æ©˜å­ï¼Ÿ" \
    --save_result "test2.json"
    # æ³¨æ„ï¼šå…©æ¬¡æ¸¬è©¦æœƒè‡ªå‹•ä½¿ç”¨ç›¸åŒçš„LoRAè¨“ç·´ç¨®å­ï¼Œç¢ºä¿çµæœå¯æ¯”è¼ƒ

ğŸ”§ ç¨®å­ä¸€è‡´æ€§é‚è¼¯ï¼ˆé‡è¦æ›´æ–°ï¼‰:

**æ–°çš„ç¨®å­é¸æ“‡å„ªå…ˆç´š**:
1. **å‘½ä»¤è¡ŒæŒ‡å®šç¨®å­**: å¦‚æœä½¿ç”¨ --seed åƒæ•¸ï¼Œç›´æ¥ä½¿ç”¨è©²å€¼
2. **LoRAè¨“ç·´ç¨®å­å„ªå…ˆ**: å¦‚æœæŒ‡å®šäº† lora_model_pathï¼Œå„ªå…ˆä½¿ç”¨LoRAè¨“ç·´æ™‚çš„ç¨®å­
3. **åŸºç¤æ¨¡å‹ç¨®å­**: åªæœ‰åœ¨æ²’æœ‰GRPOæ¨¡å‹æ™‚ï¼Œæ‰å¾åŸºç¤æ¨¡å‹ç›®éŒ„è®€å–ç¨®å­  
4. **é»˜èªç¨®å­**: ä»¥ä¸Šéƒ½æ²’æœ‰å‰‡ä½¿ç”¨43

**é—œéµæ”¹é€²**:
- âœ… **ç¢ºä¿ä¸€è‡´æ€§**: åŸºç¤æ¨¡å‹å’ŒGRPOæ¨¡å‹å§‹çµ‚ä½¿ç”¨ç›¸åŒç¨®å­
- âœ… **å…¬å¹³æ¯”è¼ƒ**: æ¶ˆé™¤éš¨æ©Ÿæ€§å·®ç•°å°æ¯”è¼ƒçµæœçš„å½±éŸ¿
- âœ… **å¯é‡ç¾æ€§**: å¤šæ¬¡é‹è¡Œç”¢ç”Ÿç›¸åŒçµæœ
- âœ… **æ™ºèƒ½é¸æ“‡**: è‡ªå‹•é¸æ“‡æœ€åˆé©çš„ç¨®å­ä¾†æº

**ä½¿ç”¨å ´æ™¯èªªæ˜**:

1. **å®Œæ•´æ¯”è¼ƒæ¨¡å¼** (æ¨è–¦):
   ```bash
   python script.py --lora_model_path "/path/to/lora"
   ```
   - åŸºç¤æ¨¡å‹å’ŒGRPOæ¨¡å‹ä½¿ç”¨ç›¸åŒçš„LoRAè¨“ç·´ç¨®å­
   - ç¢ºä¿å…¬å¹³æ¯”è¼ƒï¼Œæ¶ˆé™¤éš¨æ©Ÿæ€§å½±éŸ¿

2. **åƒ…åŸºç¤æ¨¡å‹ä½†è€ƒæ…®å¾ŒçºŒæ¯”è¼ƒ**:
   ```bash
   python script.py --only_base --lora_model_path "/path/to/lora"
   ```
   - åŸºç¤æ¨¡å‹ä½¿ç”¨LoRAè¨“ç·´ç¨®å­
   - ç‚ºå¾ŒçºŒèˆ‡GRPOæ¨¡å‹æ¯”è¼ƒåšæº–å‚™

3. **ç´”åŸºç¤æ¨¡å‹æ¸¬è©¦**:
   ```bash
   python script.py --only_base
   ```
   - ä¸æŒ‡å®šlora_model_path
   - ä½¿ç”¨åŸºç¤æ¨¡å‹è‡ªå·±çš„ç¨®å­æˆ–é»˜èªç¨®å­

4. **å¼·åˆ¶æŒ‡å®šç¨®å­**:
   ```bash
   python script.py --seed 42 --lora_model_path "/path/to/lora"
   ```
   - è¦†è“‹è‡ªå‹•ç¨®å­é¸æ“‡
   - åŸºç¤æ¨¡å‹å’ŒGRPOæ¨¡å‹éƒ½ä½¿ç”¨æŒ‡å®šç¨®å­

ğŸ“Š æ¸¬è©¦çµæœå¯ä¿¡åº¦æå‡:

ç”±æ–¼ç¢ºä¿äº†ç¨®å­ä¸€è‡´æ€§ï¼Œæ¸¬è©¦çµæœæ›´åŠ å¯é ï¼š
- âœ… **æ¶ˆé™¤éš¨æ©Ÿæ€§**: ç”Ÿæˆå·®ç•°ç´”ç²¹ä¾†è‡ªæ¨¡å‹èƒ½åŠ›å·®ç•°
- âœ… **å¯é‡ç¾çµæœ**: ç›¸åŒè¼¸å…¥ç”¢ç”Ÿç›¸åŒè¼¸å‡º
- âœ… **å…¬å¹³è©•ä¼°**: åŸºç¤æ¨¡å‹å’ŒGRPOæ¨¡å‹è™•æ–¼ç›¸åŒéš¨æ©Ÿç’°å¢ƒ
- âœ… **æº–ç¢ºåˆ†æ**: çµæ§‹åŒ–åˆ†æçµæœæ›´åŠ æº–ç¢º

âš ï¸ é‡è¦æ³¨æ„äº‹é …:

1. **ç¨®å­å„ªå…ˆç´š**: GRPOæ¨¡å‹å­˜åœ¨æ™‚å„ªå…ˆä½¿ç”¨å…¶è¨“ç·´ç¨®å­
2. **ä¸€è‡´æ€§ä¿è­‰**: åŸºç¤æ¨¡å‹æœƒè‡ªå‹•åŒæ­¥GRPOæ¨¡å‹çš„ç¨®å­
3. **è¦†è“‹æ©Ÿåˆ¶**: --seed åƒæ•¸å¯ä»¥è¦†è“‹ä»»ä½•è‡ªå‹•é¸æ“‡
4. **æ—¥å¿—æç¤º**: è…³æœ¬æœƒæ¸…æ¥šé¡¯ç¤ºä½¿ç”¨çš„ç¨®å­ä¾†æºå’Œé‚è¼¯

ğŸ’¡ æœ€ä½³å¯¦è¸:

1. **æ¯”è¼ƒæ¸¬è©¦**: è®“è…³æœ¬è‡ªå‹•é¸æ“‡LoRAè¨“ç·´ç¨®å­
2. **é‡ç¾å¯¦é©—**: è¨˜éŒ„ä½¿ç”¨çš„ç¨®å­å€¼ä»¥ä¾¿å¾ŒçºŒé‡ç¾
3. **æ‰¹é‡æ¸¬è©¦**: ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹è·¯å¾‘ç¢ºä¿ç¨®å­ä¸€è‡´
4. **çµæœåˆ†æ**: é‡é»é—œæ³¨çµæ§‹åŒ–æ”¹é€²è€Œééš¨æ©Ÿå·®ç•°
"""